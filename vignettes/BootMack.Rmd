---
title: "Non-parametric bootstrap of the Mack model"
author: "Oskar Laverny"
date: "08 September 2020"
bibliography: [BootMack.bib]
vignette: >
  %\VignetteIndexEntry{BootMack}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Description

The `BootMackChainLadder` and `MultiBootMackChainLadder` functions are based on a suggestion from [@england2002], point 7.6.5, who though that the Mack model was a certain kind of GLM but did not prove it.


A non-peer-reviewed work from [Laverny (2018)](http://www.ressources-actuarielles.net/C12574E200674F5B/0/2B139F22F51AF75FC1258306006E82F2) (in french) checks the hypothesis and conclude that Mack models is indeed a quasi-GLM. The most direct implication is that we can bootstrap it's residuals non-parametrically, and obtain a bootstrap estimate that is concordant with Mack's result (on the contrary to the Poisson, ZIP or ZIBN bootstraps), and even with [@merz2008] results.

The `BootMackChainLadder` function implement this bootstrap, with many parameter to fit the needs of the practitioner. The `MultiBootMackChainLadder` function, on the other hand, implement a multi-triangle version, where every triangles are bind to each other by a correlation link between residuals (under the hypothesis of the Braun model). The parametrisation of the function allows to specify which way this link should be taken into account. 

## The theoretical proof

Which actuary does not know about **Mack's model** ? Due to [@Mack1991], this model is fairly simple.

Suppose you have a (cumulative) triangle, and denote $C_{i,j}$ the value of this triangle from the $i$'th row and the $j$'th collumn.

The Mack model consist of a stochastic model that enframe the classical chain-ladder estimator, denoted here by the vector $\mathbf{\hat{f}}$, defined $\forall j \in \{1,...,n\}$ by :

$$\hat{f}_j = \frac{\sum\limits_{i=1}^{n-j+1} C_{i,j+1}}{\sum\limits_{i=1}^{n-j+1} C_{i,j}}$$


Then we estimate future claims by setting, $\forall i,j \,|\, i+j>n$, $C_{i,j+1} = \hat{f_j} C_{i,j}$, which is the chain-ladder estimate.

Mack then found stochastic hypothesis that make those estimator _better_ in a certain sense (in the MSE-sense) : 

* $(H1)$ : The lines are independents
* $(H2)$ : $\exists\, \mathbf{f} \text{ such that }  \forall i,j,\, \mathbb{E}(C_{i,j+1}\,|\, C_{i,1},...,C_{i,j}) = f_j C_{i,j}$
* $(H3)$ : $\exists\, \mathbf{\sigma} \text{ such that }  \forall i,j,\, \mathbb{V}(C_{i,j+1}\,|\, C_{i,1},...,C_{i,j}) = \sigma_j^2 C_{i,j}$

First, we should note the conditioning of $C_{i,j+1}$ on the past of the origin year. 

Suppose that, following [@england2002], we reformulate the model in term of individual developments factors. Set : 

$$F_{i,j} = \frac{C_{i,j+1}}{C_{i,j}} \text{ and } w_{i,j} = C_{i,j}$$

The two last hypothesis can then be reformulated in the following way (just dividing by $w_{i,j}$).

* $(H2*)$ : $\exists\, \mathbf{f} \text{ such that }  \forall i,j,\, \mathbb{E}(F_{i,j}\,|\, w_{i,j}) = f_j$
* $(H3*)$ : $\exists\, \mathbf{\sigma} \text{ such that }  \forall i,j,\, \mathbb{V}(F_{i,j}\,|\, w_{i,j}) = \frac{\sigma_j^2}{w_{i,j}}$

Note that i assume that the information given by the past is well summarized by $w_ {i,j}$, which is indeed the case under those hypothesis. 

Look again at the formula for the estimator of the chain-ladder development factors. Do you see the weighted mean of $F$'s, weighed by $C$'s ? Indeed, since we posed $w_{i,j} = C_{i,j}$, 


$$\hat{f}_j = \frac{\sum\limits_{i=1}^{n-j+1} F_{i,j}\, w_{i,j}}{\sum\limits_{i=1}^{n-j+1} w_{i,j}}$$

Recall now Mack's estimators for the variance parameters $\mathbf{\sigma}$ : 


$$\hat{\sigma}_j = \frac{1}{n-j} \sum\limits_{i=1}^{n-j+1} w_{i,j} (F_{i,j} - \hat{f}_j)^2$$

they are simply an unbiased variance estimator, for the estimation of $F$'s by $\hat{f}$'s, with case weights $w$'s. Although the normalization is not right (should be $\frac{1}{n-j-1}$), they do. 

We will now found the corresponding GLM. To estimate the first order, i.e $f$'s, we just take a weighted average of observation $F$'s, with weights $w$'s, by column. This GLM has the following specifications:

* Link function Identity 
* Any law in the exponential family
* case weights given by $w$
* response variable $F$
* Only covariable: the indicatrix of the column

Here is an exemple on the ABC triangle (incured triangle -- cumulative view):

```{r echo=FALSE}
library(ChainLadder)
data(ABC)
print(ABC)
```

First, the estimated development factor given by standard chain-ladder : 

```{r trying_first_order}
MackChainLadder(ABC)$f
```


If we create a data.frame with what we need and then try a GLM (here the function `lm` is enough, as the variance hypothesis is Normal-like) on it : 


```{r trying_first_order2}
df <- as.data.frame(ABC)
df$weight <- as.vector(cbind(rep(NA, nrow(ABC)), as.matrix(ABC))[, seq_len(nrow(ABC))])
df$F <- df$value / df$weight
df <- df[complete.cases(df), ]

model <- lm(data = df, formula = F ~ factor(dev) + 0, weights = weight)

df$f.hat <- model$fitted.values
df$rez2 <- df$weight * (df$F - df$f.hat)^2
print(head(df))
```

I also extracted the squared Pearson residuals. Indeed, if we want to model $\sigma$'s too, we nieed a second model to calculate the weighted average of Squared Pearson residuals, which is standard in the context of Joint modeling of Glm's, see e.g [@mccullagh1989] for a very good exposition. 

To calculate the weighted average of squared residuals (for the dispersion parameters), we use another linear model since only the first order is here of interest. Note that we exclude the last column of the triangle, since there is only one point (and therefore no estimation of variance is possible). We also add the last value estimated by Mack for sake of completeness. 

```{r suite}


model2 <- lm(data = df[df$dev != nrow(ABC), ], formula = rez2 ~ factor(dev))

fit <- model2$fitted.values

df$estimated.phi <- c(fit, min(fit[length(fit)], fit[length(fit) - 1], fit[length(fit)]^2 / fit[length(fit) - 1]))

print(head(df))
```

And there we have it ! Those estimates are clearly the same as Mack's. Indeed : 

```{r}
model4 <- MackChainLadder(ABC, est.sigma = "Mack")

results <- data.frame(dev = unique(df$dev))
results$phi <- 0
for (dev in results$dev) {
  results[results$dev == dev, "phi"] <- mean(df[df$dev == dev, "estimated.phi"])
}

results$degree.of.freedom <- max(results$dev) - results$dev
xxx <- (results$phi * (results$degree.of.freedom + 1) / (results$degree.of.freedom))[1:(length(results$phi) - 1)]
results$phi <- c(xxx, min(xxx[length(xxx)], xxx[length(xxx) - 1], xxx[length(xxx)]^2 / xxx[length(xxx) - 1]))
results$f <- model$coefficients[1:(dim(ABC)[1] - 1)]
results$f.cl <- model4$f[1:(dim(ABC)[1] - 1)]
results$mack <- model4$sigma^2

results$dev <- NULL
print(results)
```

We reproduced Mack's result in a new model. The hypothesis we used are : 

* $(H1**)$ : The lines are independants
* $(H2**)$ : $\exists\, \mathbf{f}\text{ and }\mathbb{\sigma} \text{ such that }  \forall i,j,\, F_{i,j} |\, w_{i,j} \sim \mathcal{N}(f_j, \frac{\sigma_j^2}{w_{i,j}})$

Well, if we forget about the law assumption and only use quasi-Glm (see, once again, [@mccullagh1989]), those are Mack hypothesis. Therefore, Mack's model is a quasi-Glm, conditional, and weighted, with a normal-like variance assumption. 

Some remarks : 

- Note that we played a little with degree of freedom : Since we are estimating the $\sigma$'s _after_ the $f$'s, we should take into account that $n-1$ parameters are already estimated. This is classical in the joint modeling process. 
- Furthermore, the joint modeling process gives the choice of residuals (pearson residuals or deviance contribution), but in a the normal-like case they are equal so this degree of freedom is not present here. 
- Last but not least, the accointance for the kurtosis is not necessary here because of the normal variance assumption. 

The following `check_model` function summarises this process and can be used to check the claim that Mack's model is a GLM on another triangle:

```{r }
check_model <- function(triangle) {
  df <- as.data.frame(triangle)
  df$weight <- as.vector(cbind(rep(NA, nrow(triangle)), as.matrix(triangle))[, seq_len(nrow(triangle))])
  df$F <- df$value / df$weight
  df <- df[complete.cases(df), ]

  model <- lm(data = df, formula = F ~ factor(dev) + 0, weights = weight)

  df$f.hat <- model$fitted.values
  df$rez2 <- df$weight * (df$F - df$f.hat)^2

  model2 <- lm(data = df[df$dev != nrow(triangle), ], formula = rez2 ~ factor(dev))

  fit <- model2$fitted.values
  df$estimated.phi <- c(fit, min(fit[length(fit)], fit[length(fit) - 1], fit[length(fit)]^2 / fit[length(fit) - 1]))

  model3 <- lm(data = df, formula = F ~ factor(dev) + 0, weights = weight / estimated.phi)
  model4 <- MackChainLadder(triangle, est.sigma = "Mack")

  results <- data.frame(dev = unique(df$dev))
  results$phi <- 0
  for (dev in results$dev) {
    results[results$dev == dev, "phi"] <- mean(df[df$dev == dev, "estimated.phi"])
  }
  results$degree.of.freedom <- max(results$dev) - results$dev
  xxx <- (results$phi * (results$degree.of.freedom + 1) / (results$degree.of.freedom))[1:(length(results$phi) - 1)]
  results$phi <- c(xxx, min(xxx[length(xxx)], xxx[length(xxx) - 1], xxx[length(xxx)]^2 / xxx[length(xxx) - 1]))
  results$f <- model$coefficients[1:(dim(triangle)[1] - 1)]
  results$f.cl <- model4$f[1:(dim(triangle)[1] - 1)]
  results$mack <- model4$sigma^2
  results$dev <- NULL


  return(list(mod.f = model, mod.phi = model2, mod.final = model3, rez = results))
}


data(ABC)
print(check_model(ABC)$rez)
```



The categorization of Mack hypothesis set as a GLM allows several things : 

* The use of standard statistical tools : cook's distance, residual analysis, etc...
* The use of standard software : R's glm function is probably more safe than `ChainLadder`'s specific function. 
* The simplicity of extension : The Bornhutter-Fergusson extension, e.g, can be easily introduced within a GLM framework, it's harder within Mack's framework. 
* The bootstrapping of residuals without distributional assumptions (they are just supposed to be i.i.d).



## The implementation 

The package exports several functions related to this problematic. The most important one is the `BootMackChainLadder` function, which implements a standard bootstrap fo residuals for this GLM. Note that these residuals are bare Mack residuals, not Poisson's residuals, and hence the results obtained by this bootstrap are centered on Mack's results. 

Furthermore, the model allows for a Next year bootstrapping, standardized on Merz-Wuthrich results, as the following tests shows: 


```{r }
data(ABC)
model <- BootMackChainLadder(ABC, B = 1000, distNy = "normal")
mack <- MackChainLadder(ABC)
print(CDR(model))
print(CDR(mack)[, 1:2])
```

The `distNy` option is used to specify if we want a distribution for computation of the CDR. Specifying 'normal' will give results that are convergent to the Merz & Wuthrich formula, as we just checked. Specifiying 'residuals', on the other hand, will give results that are coherent with Mack's hypothesis. 

Increasing the number of resamples will make the convergence better, but by digging into the `model` object, we can obtain more details and see every Bootstrap run independently from each other.

Another function exported by the package gives the possibility to do the same kind of bootstrap in the Braun model, see [@braun] and [@prohl]: This is a multi-line reserving framework where each triangles are supposed to have joint residuals. 

For empirical residuals from the Mack model, this corresponds to a joint resampling of the residuals attribution across lines of buisness. Under a normal assumption from the residuals, we get back the mulit-line framework that was developped by Merz & Wurthrich in [@merz2007], where the residuals are jointly normal with an estimated covariance structure taken from the Braun model. 

Both these models are avaliable under the `MultiBootMackChainLadder` function, which can be applied as follows: 

```{r }
data(ABC)
Triangles <- list(ABC, ABC)
model <- MultiBootMackChainLadder(Triangles, B = 1000)
print(CDR(model))
print(mean(model))
```


__References__

